{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMP_data_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmrmas/SRAMicrobiomeCategoryPipeline/blob/sam/HMP_data_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j04QL-TPJy1"
      },
      "source": [
        "\n",
        "# HMP data training for model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EFbQj6OaG_m"
      },
      "source": [
        "!pip install -Uqq fastbook waterfallcharts treeinterpreter dtreeviz #kaggle \n",
        "!pip install -q unpackai"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azcl-ikfOQJR"
      },
      "source": [
        "from fastbook           import *\n",
        "from unpackai.tabular   import no_missing_values, plot_hist\n",
        "\n",
        "from pandas.api.types   import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "from fastai.tabular.all import *\n",
        "from sklearn.ensemble   import RandomForestRegressor\n",
        "from sklearn.ensemble   import RandomForestClassifier\n",
        "from sklearn.tree       import DecisionTreeRegressor\n",
        "from dtreeviz.trees     import *\n",
        "from IPython.display    import Image, display_svg, SVG\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.max_columns = 8"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-prUftUtT_u"
      },
      "source": [
        "\n",
        "**Machine Learning Task**:  Categorize micrbiome profiles based on their origin (from the HMP dataset) \n",
        "\n",
        "**Target**: categories\n",
        "\n",
        "**Samples**: data samples from individuals\n",
        "\n",
        "**Features**: taxon \n",
        "\n",
        "**Values**: frequency data of the microbiome, will be normalized against \"Bacteria\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkUg9HF3B-Vi"
      },
      "source": [
        "#### 1. Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVdx1TChARj"
      },
      "source": [
        "Download the file from dropbox, where features are taxa, disease indiction and tissue source. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih8V-H1buLP0"
      },
      "source": [
        "df = pd.read_csv(\"https://www.dropbox.com/s/jgeae300bbjpzm5/HMPFeatureTable.csv?dl=1\", sep=',')"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sG0hdDhYXH"
      },
      "source": [
        "Explore the dataset to choose the column name for setting a target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rS8EQThhPcN"
      },
      "source": [
        "One could select a particular tissue source to dive into"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtK3HdEITitl"
      },
      "source": [
        "#select the tissue\n",
        "#df = df[df['categories'] == 'HMP_ForegutEsophagealAdenocarcinoma']\n",
        "#df = df[df['affected'] != 'Yes-Normal'] "
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "988_vCXmrxBi"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNvH9qOhYR5"
      },
      "source": [
        "At the moment the target feature still has to be createdd. It will be a concatenation of the tissue origin and the disease indication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oaa1U45AwN7y"
      },
      "source": [
        "targetFeature = 'affected_categories'                                            \n",
        "#removeFeature = 'affected'"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJnYwcdwbsC"
      },
      "source": [
        "first get rid of the columns that are not used (categories or  affected)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEYwJ4XfBXC8"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df['affected_categories'] = df['categories'] + '_' + df['affected'].apply(str)  \n",
        "target = [targetFeature]  \n",
        "df = df.drop(['categories', 'affected'],axis=1)      \n",
        "#df = df.drop([removeFeature],axis=1)               #include this for instance when only 'categories' are calssified             \n",
        "df[targetFeature]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unWOuktIPDMq"
      },
      "source": [
        "#check our categories and show how many we have of each\n",
        "categories = list(set(df[target]))\n",
        "size_per_category = pd.pivot_table(df, index = targetFeature, values = 'Bacteria', aggfunc = [len])  \n",
        "list(size_per_category.index.values)\n",
        "graph_df = pd.DataFrame({'lab':list(size_per_category.index.values), 'count':list(size_per_category.iloc[0:,0])})\n",
        "ax = graph_df.plot.bar(x='lab', y='count', rot=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-DZUnt9PKnl"
      },
      "source": [
        "categories = list(set(df[target]))\n",
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Yhf2dyUMl1"
      },
      "source": [
        "size_per_category "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhE3H81b7ox"
      },
      "source": [
        "categs = list(size_per_category.iloc[0:,0])\n",
        "categs.sort()\n",
        "minval = int(categs[0])\n",
        "print (minval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssm33c_Fyr82"
      },
      "source": [
        "At the moment let's include categories with more then 76 observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISoaKBEbCkx"
      },
      "source": [
        "minval = 76\n",
        "#need to get equal number of categories\n",
        "slimmed_down = list()\n",
        "for s in list(size_per_category.index.values):\n",
        "  this_sample = df[df[targetFeature]==s]               \n",
        "  if (len(this_sample) > minval-1):\n",
        "    l = list(range(len(this_sample)))\n",
        "    include = random.sample(l, minval)\n",
        "    include_this = this_sample.iloc[include]\n",
        "    slimmed_down.append(include_this)\n",
        "df = pd.concat(slimmed_down)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8XrD5wdfa2P"
      },
      "source": [
        "Here we normalize all values against the \"Bacteria\" taxon in order to align samples of differnt origin better, and to be able to export and to be able to re-use the  model as a .pt model later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s3t5FnimTIP"
      },
      "source": [
        "#divide through \"Bacteria\" value\n",
        "targetCol = df[target]\n",
        "dfDrop = df.drop(target,axis=1) #remove the 'category' column"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzE0kuSGS_GT"
      },
      "source": [
        "df = dfDrop.iloc[:,:].div(df.Bacteria, axis=0) #divide everything by the bacteria value "
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je0neaXaRoq9"
      },
      "source": [
        "df = dfDrop.join(targetCol) #bring back the target"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6uQjLZjMfw"
      },
      "source": [
        "Utilize `cont_cat_split` to derive continious and categorival variables (not explicitely needed in this type of datasets though)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6_pmlUDxhK8"
      },
      "source": [
        "cont_var, cat_var = cont_cat_split(df, dep_var=target)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9RODqUV4ni"
      },
      "source": [
        "#### 2. Data Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOM1JYvZPWhv"
      },
      "source": [
        "cat_var #just checkin'!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLgtNNHGP4e9"
      },
      "source": [
        "splits=RandomSplitter(valid_pct=0.3)(df)\n",
        "splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ6pNhp6P4VB"
      },
      "source": [
        "df = no_missing_values(df, 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYlyFdBz_Oil"
      },
      "source": [
        "df = df.fillna(0) #get rid of missing values"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZU-G9oJ2Zxa"
      },
      "source": [
        "Setting up the Random Forest. Parameters can probably be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgV37jhEP4Dr"
      },
      "source": [
        "#@title Random Forest Functionality\n",
        "\n",
        "def rf_feat_importance(xs, y, valid_xs, valid_y):\n",
        "    \"\"\"Fit a RF model on the passed xs and y, and evaluate oob and validation R2 score\"\"\"\n",
        "    print(\"Setting up model ...\")\n",
        "    n_estimators = 40\n",
        "    max_samples = min(50_000, xs.shape[0])\n",
        "    max_features = 0.5\n",
        "    min_samples_leaf = 5\n",
        "    m = RandomForestClassifier(n_estimators=n_estimators, \n",
        "                              max_samples=max_samples, \n",
        "                              max_features=max_features, \n",
        "                              min_samples_leaf=min_samples_leaf, \n",
        "                              oob_score=True,\n",
        "                              n_jobs=-1,\n",
        "                              random_state=88)\n",
        "\n",
        "    print(\"Fitting model ...\")\n",
        "    m.fit(X=xs, y=y)\n",
        "    oob_r2 = m.oob_score_\n",
        "    val_r2 = m.score(valid_xs, valid_y)\n",
        "\n",
        "    return pd.DataFrame({'cols':xs.columns, 'imp':m.feature_importances_}\n",
        "                        ).sort_values('imp', ascending=True)\n",
        "\n",
        "def get_oob_R2(xs, y, c=[]):\n",
        "    \"\"\"Fits a RF model and evaluate a OOB score on xs where features c are dropped\n",
        "    params: \n",
        "        xs: the training set\n",
        "        y:  the targets for training\n",
        "        c:  (optional): the name of one column or a list of columns to drop from xs\n",
        "    \"\"\"\n",
        "    if isinstance_str(c, 'str'): c = [c]\n",
        "    if set(c).difference(set(xs.columns)) != set():\n",
        "        return None\n",
        "    n_estimators = 40\n",
        "    max_samples = min(50_000, xs.shape[0])\n",
        "    max_features = 0.5\n",
        "    min_samples_leaf = 5\n",
        "    m = RandomForestClassifier(n_estimators=n_estimators, \n",
        "                              max_samples=max_samples, \n",
        "                              max_features=max_features, \n",
        "                              min_samples_leaf=min_samples_leaf, \n",
        "                              oob_score=True,\n",
        "                              n_jobs=-1,\n",
        "                              random_state=88)\n",
        "    m.fit(X=xs.drop(c, axis=1), y=y)\n",
        "    oob_r2 = m.oob_score_\n",
        "    #print(f\"OOB R2 Score:\")\n",
        "    return oob_r2\n",
        "\n",
        "#Before we can run the random forest, we also need to define the DataBlock.\n",
        "to = TabularPandas(df, \n",
        "                   procs=[Categorify, FillMissing, Normalize],\n",
        "                   cat_names=cat_var,\n",
        "                   cont_names=cont_var,\n",
        "                   y_names=target,\n",
        "                   splits=splits)\n",
        "\n",
        "xs, y = to.train.xs, to.train.y #training set\n",
        "valid_xs, valid_y = to.valid.xs, to.valid.y #validation set\n",
        "\n",
        "get_oob_R2(xs, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaYtVmFlP3jE"
      },
      "source": [
        "#let's look at the feature importance\n",
        "fi = rf_feat_importance(xs, y, valid_xs, valid_y)\n",
        "fi[:]\n",
        "#len(fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfoiH3yQi39f"
      },
      "source": [
        "keep only those above a certain threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td3JtzZvf6VN"
      },
      "source": [
        "to_keep = fi[fi.imp>0.0001].cols\n",
        "xs_imp = xs[to_keep]\n",
        "valid_xs_imp = valid_xs[to_keep]\n",
        "to_keep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh-xYpHKgN0U"
      },
      "source": [
        "get_oob_R2(xs_imp, y)\n",
        "#miraculously this may DEPEND ON THE ORDER of the data...?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho8GypAlkFSU"
      },
      "source": [
        "cluster_columns(xs_imp, figsize=(12, 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-8pWAn8kbTo"
      },
      "source": [
        "# Try to remove features that reduce the oob\n",
        "# simply remove the feature that leads to maximal oob improvement when it is removed\n",
        "# probably not perfect to reach global oob minimum but hopefully reasonable to improve the feature selection\n",
        "\n",
        "#frst set the status quo\n",
        "prev_oob = get_oob_R2(xs_imp, y)\n",
        "\n",
        "while (1==1): #loop untill the new oob value does not increase any longer by removing features\n",
        "  #loop through the fetaures and remove the feature with the lowest oob if it is lower than the new oob\n",
        "  new_oobs = []\n",
        "  for c in (cat_var):\n",
        "    temp_oob = get_oob_R2(xs_imp, y, c)\n",
        "    if (temp_oob != None):\n",
        "      new_oobs.append([c , temp_oob])\n",
        "  for c in (cont_var):\n",
        "    temp_oob = get_oob_R2(xs_imp, y, c)\n",
        "    if (temp_oob != None):\n",
        "      new_oobs.append([c , temp_oob])\n",
        "\n",
        "  #check if the lowest value can be removed: only when it makes the oob higher\n",
        "  new_oobs = sorted(new_oobs,key=lambda x: (x[1]), reverse= True)\n",
        "  new_oob_value = new_oobs[0][1]\n",
        "\n",
        "  print(f\"new OOB: {new_oob_value} from feature {new_oobs[0][0]} old OOB: {prev_oob}\")\n",
        "  if (new_oob_value  > prev_oob): #clean up the features\n",
        "    xs_imp = xs_imp.drop(labels = new_oobs[0][0], axis = 1) \n",
        "    valid_xs_imp = valid_xs_imp.drop(labels = new_oobs[0][0], axis = 1)\n",
        "    prev_oob = new_oob_value\n",
        "  else:\n",
        "    break  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gnSg5Ej-uRA"
      },
      "source": [
        "cluster_columns(xs_imp, figsize=(12, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5BBEvXlifh"
      },
      "source": [
        "keys = xs_imp.keys()\n",
        "cont_var_final, cat_var_final  = cont_cat_split(xs[keys])"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG1WS-GC-ac3"
      },
      "source": [
        "cont_var_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnWZcvcbDoms"
      },
      "source": [
        "to = TabularPandas(df, \n",
        "                   procs=[Categorify,FillMissing,Normalize],\n",
        "                   cont_names=cont_var_final,\n",
        "                   y_names=target,\n",
        "                   y_block=CategoryBlock,\n",
        "                   splits=splits)\n",
        "\n",
        "xs_final, y = to.train.xs, to.train.y\n",
        "valid_xs_final, valid_y = to.valid.xs, to.valid.y\n",
        "\n",
        "#dls = to.dataloaders(bs=1024)\n",
        "dls = to.dataloaders()"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1-rEuOWr2QP"
      },
      "source": [
        "#### 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoRU_z5qLcxL"
      },
      "source": [
        "learn = tabular_learner(dls, metrics=accuracy)"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGrzNF1gZ2_Y"
      },
      "source": [
        "splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puqZULFLD-cy"
      },
      "source": [
        "learn.fit_one_cycle(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UEL3siOsV2i"
      },
      "source": [
        "#### 5. Interpret the model and make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbNZWfaDLfEP"
      },
      "source": [
        "outcome = learn.show_results(dl=dls.valid, max_n=20)\n",
        "outcome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lkES_sKEV-m"
      },
      "source": [
        "row, clas, probs = learn.predict(df.iloc[2])\n",
        "print(probs)\n",
        "print(learn.dls.vocab)\n",
        "row.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO710G4hEuz-"
      },
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "# confusion matrix as an array\n",
        "print(interp.confusion_matrix())\n",
        "\n",
        "# confusion as a plot\n",
        "interp.plot_confusion_matrix( figsize=(12, 12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2rZZf_9TVpT"
      },
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYO6J8LwTU8s"
      },
      "source": [
        "# Save the entire model as a pkl (to re-use in this script) and pt (to use as a standalone off-line).\n",
        "!mkdir -p drive/MyDrive/saved_model/\n",
        "learn.export('drive/MyDrive/saved_model/Model.pkl') \n",
        "pd.Series(xs_imp.keys()).to_csv('drive/MyDrive/saved_model/xses.csv', index=False, header=False) "
      ],
      "execution_count": 276,
      "outputs": []
    }
  ]
}